[GENERAL]
model_type = student
save_model = /home/users/sgarda/cache/tensorboard-logdir/st_qd/st1_qd_w2l
input_channels = 39
data_format = channels_last
conv_type = conv
quantize = True

[FILES]
vocab_path = /home/users/sgarda/project/tfrecord_data/ctc_vocab.txt
train_data = /home/users/sgarda/project/tfrecord_data/tfrecords_mfccs.train
eval_data = /home/users/sgarda/project/tfrecord_data/tfrecords_mfccs.dev-clean
test_data = /home/users/sgarda/project/tfrecord_data/tfrecords_mfccs.test-clean
teacher_logits = /home/users/sgarda/project/tfrecord_data/w2l_v1.logits

[TRAIN]
batch_size = 64
epochs = 10
activation = elu
bn = false
clipping = 0
alpha = 0.5
temperature = 5
adam_lr = 1e-4
adam_eps = 1e-8

[QUANTIZATION]
num_bits = 8
bucket_size = 256
stochastic = False

[LM]
lm = False
beam_search = False
knlem_op = ../kenlm/kenlm_op/libctc_decoder_with_kenlm.so
lm_binary = ../kenlm/kenlm_data/lm.binary
lm_trie = ../kenlm/kenlm_data/trie
lm_alphabet = ../kenlm/kenlm_data/alphabet.txt
lm_weight = 0.8
word_count_weight = 1.00
valid_word_count_weight = 2.3
top_paths = 1
beam_width = 100

[STUDENT]
filters = [250,250, 250, 250, 1000, 1000]
widths = [ 48,7, 7, 7, 32, 1]
strides = [2,1, 1, 1, 1, 1]
dropouts = [0,0, 0, 0, 0, 0]


