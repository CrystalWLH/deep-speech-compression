[GENERAL]
model_type = teacher
save_model = /home/users/sgarda/cache/tensorboard-logdir/pm_quant_w2l_v1
input_channels = 39
data_format = channels_last
conv_type = conv

[FILES]
vocab_path = /home/users/sgarda/project/tfrecord_data/ctc_vocab.txt
train_data = /home/users/sgarda/project/tfrecord_data/tfrecords_mfccs.train
eval_data = /home/users/sgarda/project/tfrecord_data/tfrecords_mfccs.dev-clean
test_data = /home/users/sgarda/project/tfrecord_data/tfrecords_mfccs.test-clean

[TRAIN]
batch_size = 64
epochs = 20
activation = elu
bn = false
clipping = 0
adam_lr = 1e-4
adam_eps = 1e-8

[LM]
lm = False
beam_search = False
knlem_op = ../kenlm/kenlm_op/libctc_decoder_with_kenlm.so
lm_binary = ../kenlm/kenlm_data/lm.binary
lm_trie = ../kenlm/kenlm_data/trie
lm_alphabet = ../kenlm/kenlm_data/alphabet.txt
lm_weight = 0.8
word_count_weight = 1.00
valid_word_count_weight = 2.3
top_paths = 1
beam_width = 100

[TEACHER]
filters = [250,250, 250, 250, 250, 250, 250, 250, 2000, 2000]
widths = [ 48,7, 7, 7, 7, 7, 7, 7, 32, 1]
strides = [2,1, 1, 1, 1, 1, 1, 1, 1, 1]
dropouts = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]





